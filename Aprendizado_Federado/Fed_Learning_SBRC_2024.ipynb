{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "URS8INPtwC9h"
      },
      "source": [
        "Sumário\n",
        "* [RNN](#rnn)\n",
        "* [Server](#server)\n",
        "* [Base Station](#base-station) - [Local](#bs-local)\n",
        "* [User](#user)\n",
        "* [Dataset](#dataset)\n",
        "* [Create BSs, Users and Server](#create_bs_us)\n",
        "* [Predicting](#predicting)\n",
        "* [Training](#training)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c2w1NVkgsm_1"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y9cYym_Qtrpc"
      },
      "source": [
        "Importação de bibliotecas, definição de hiperparâmetros e classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WS7GPY4Ltuav"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import copy\n",
        "import math\n",
        "from scipy.spatial import distance\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn.functional as F\n",
        "import csv\n",
        "\n",
        "# Set the device to use\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "print(\"Device -> \", device)\n",
        "\n",
        "# Define the hyperparameters\n",
        "num_features = 3\n",
        "input_size = num_features\n",
        "hidden_size = 66\n",
        "output_size = num_features\n",
        "learning_rate = 0.001\n",
        "rounds = 1000\n",
        "epochs = 40\n",
        "torch_seed = 0\n",
        "\n",
        "num_base_stations = 5   #Number of Base Stations\n",
        "num_users         = 20  #Number of users\n",
        "\n",
        "rows_per_partition = 3600  #1 hour (3600 seconds) per user\n",
        "num_user_regs = 5          #Number of regs (location and orientation) received by round\n",
        "num_user_regs_per_time = 1 #1 reg with location and orientation by second\n",
        "accuracy_threshold = 99.5  #Model accuracy threshold\n",
        "\n",
        "dir_base = '/content/drive/My Drive/SBRC 2024/'\n",
        "\n",
        "gru_training_model     = 1\n",
        "esn_training_model     = 2\n",
        "lstm_training_model    = 3\n",
        "\n",
        "current_training_model = gru_training_model\n",
        "\n",
        "fitness_filter = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sgr8SGvIuvTo"
      },
      "source": [
        "<a name=\"rnn\"></a>\n",
        "Definindo redes neurais recorrente (RNN):\n",
        "* 3 neurônios na camada de entrada (input)\n",
        "* 66 neurônios na camada intermediária (hidden)\n",
        "* 3 neurônios na camada de saída (output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FjXJ5vJv8eV_"
      },
      "outputs": [],
      "source": [
        "class GRUModel(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(GRUModel, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.rnn = nn.GRU(input_size, hidden_size, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Initialize hidden state with zeros\n",
        "        h = torch.zeros(1, x.shape[0], self.hidden_size).to(device)\n",
        "\n",
        "        # Forward propagate RNN\n",
        "        out, h = self.rnn(x, h)\n",
        "\n",
        "        # Decode hidden state of last time step\n",
        "        out = self.fc(out[:, -1, :])\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YCSLYEe3GXoy"
      },
      "outputs": [],
      "source": [
        "class LSTMModel(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
        "        super(LSTMModel, self).__init__()\n",
        "        self.num_layers  = num_layers\n",
        "        self.hidden_size = hidden_size\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Initialize hidden state\n",
        "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
        "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        out, _ = self.lstm(x, (h0, c0))\n",
        "\n",
        "        # Use the output at the last time step\n",
        "        out = self.fc(out[:, -1, :])\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tHktZMMt11Xs"
      },
      "outputs": [],
      "source": [
        "class EchoStateNetwork(nn.Module):\n",
        "    def __init__(self, input_size, reservoir_size, output_size, connectivity=0.1):\n",
        "        super(EchoStateNetwork, self).__init__()\n",
        "        self.input_size = input_size\n",
        "        self.reservoir_size = reservoir_size\n",
        "        self.output_size = output_size\n",
        "\n",
        "        # Define layers using nn.Linear\n",
        "        self.input = nn.Linear(input_size, reservoir_size, bias=False)\n",
        "        self.reservoir = nn.Linear(reservoir_size, reservoir_size, bias=False)\n",
        "        self.output = nn.Linear(reservoir_size, output_size, bias=False)\n",
        "\n",
        "        # Initialize reservoir weights\n",
        "        nn.init.xavier_uniform_(self.input.weight)\n",
        "        nn.init.xavier_uniform_(self.reservoir.weight)\n",
        "        nn.init.xavier_uniform_(self.output.weight)\n",
        "\n",
        "        # Sparsify the reservoir weights\n",
        "        mask = (torch.rand(reservoir_size, reservoir_size) < connectivity).float()\n",
        "        self.reservoir.weight.data *= mask\n",
        "\n",
        "    def forward(self, input_data):\n",
        "      predictions = []\n",
        "      X = torch.zeros(input_data.size(0), self.reservoir_size, device=input_data.device)  # Reservoir activations\n",
        "\n",
        "      for inp in input_data.unbind(1):  # Unbind along the time steps\n",
        "          X = torch.tanh(self.input(inp) + self.reservoir(X))\n",
        "          prediction = self.output(X)\n",
        "          predictions.append(prediction)\n",
        "\n",
        "      return torch.cat(predictions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D-vyYxIZAvPr"
      },
      "source": [
        "<a name=\"server\"></a>\n",
        "Definindo uma classe que representa o servidor (Server) da federação (Federated Learning)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "49bp_soYA8-g"
      },
      "outputs": [],
      "source": [
        "class Server():\n",
        "    def __init__(self, num_users):\n",
        "      self.users_global_model = {}\n",
        "      self.users_global_model_accuracy = {}\n",
        "\n",
        "    def update_global_model(self, base_stations):\n",
        "\n",
        "      for us_id in range(num_users):\n",
        "\n",
        "        #Only those models who needs will be be trainned\n",
        "        if(us_id in self.users_global_model_accuracy and\n",
        "           self.users_global_model_accuracy[us_id] >= accuracy_threshold):\n",
        "          continue\n",
        "\n",
        "        us_models = []\n",
        "        us_models_accuracy = []\n",
        "\n",
        "        #Getting the last User's global model (if exists)\n",
        "        if (us_id in self.users_global_model):\n",
        "          us_models.append(self.users_global_model[us_id])\n",
        "\n",
        "        #Getting models of the same User in different BS\n",
        "        for bs in base_stations:\n",
        "          us_models.append(bs.users[us_id].model)\n",
        "          us_models_accuracy.append(bs.users[us_id].model_accuracy)\n",
        "\n",
        "        #Models with too lower accuracy will not be aggregated\n",
        "        if(fitness_filter):\n",
        "          self._gather_models_capable_to_aggregation(us_models_accuracy, us_models)\n",
        "\n",
        "        #Setting the user's global model through the new global model for one more round\n",
        "        self.users_global_model[us_id] = self._aggregate_user_global_model(us_id, us_models)\n",
        "\n",
        "        #Check model's accuracy\n",
        "        self.users_global_model_accuracy[us_id] = sum(us_models_accuracy) / len(us_models_accuracy)\n",
        "\n",
        "        #Updating the User in each Base Station with the global model\n",
        "        for bs in base_stations:\n",
        "            bs.users[us_id].model = self.users_global_model[us_id]\n",
        "\n",
        "    def _gather_models_capable_to_aggregation(self, us_models_accuracy, us_models):\n",
        "      mean_accuracy = np.mean(us_models_accuracy)\n",
        "      std_accuracy = np.std(us_models_accuracy)\n",
        "\n",
        "      for i, accuracy in enumerate(us_models_accuracy):\n",
        "        if( mean_accuracy - accuracy > std_accuracy):\n",
        "          del us_models[i]\n",
        "          del us_models_accuracy[i]\n",
        "\n",
        "    def _aggregate_user_global_model(self, us_id, us_models):\n",
        "      with torch.no_grad():\n",
        "        # Initializing a new model with the same architecture and parameter shapes\n",
        "        new_model = copy.deepcopy(us_models[0])\n",
        "\n",
        "        if(current_training_model == esn_training_model):\n",
        "          local_output_layers = []\n",
        "\n",
        "          # Collect output layer parameters of the local models\n",
        "          for model in us_models:\n",
        "              local_output_layers.append(model.output.weight.data.clone())\n",
        "\n",
        "          # Aggregate only the output layer parameters using FedAVG\n",
        "          avg_output_params = sum(local_output_layers) / len(us_models)\n",
        "          new_model.output.weight.data = avg_output_params.clone()\n",
        "        else:\n",
        "          # Zero out all of the parameters in new_model\n",
        "          for param in new_model.parameters():\n",
        "              param.data *= 0\n",
        "\n",
        "          # Sum up the parameter tensor values from all models\n",
        "          for model in us_models:\n",
        "              for param, new_param in zip(model.parameters(), new_model.parameters()):\n",
        "                  new_param.data += param.data\n",
        "\n",
        "          # Compute the average (FedAVG) by dividing by the number of models\n",
        "          for param in new_model.parameters():\n",
        "              param.data /= len(us_models)\n",
        "\n",
        "      # Model updated\n",
        "      return new_model\n",
        "\n",
        "    def can_stop_federation(self):\n",
        "      has_achieved_needed_accuracy = 0\n",
        "\n",
        "      for accuracy in self.users_global_model_accuracy.values():\n",
        "        if(accuracy >= accuracy_threshold):\n",
        "          has_achieved_needed_accuracy += 1\n",
        "\n",
        "      return True if len(self.users_global_model_accuracy.items()) == has_achieved_needed_accuracy else False\n",
        "\n",
        "    def print(self):\n",
        "      for k, v in self.users_global_model_accuracy.items():\n",
        "        print(\"User {} model's mean accuracy = {} [{}]\".format(k, v, \"READY\" if v >= accuracy_threshold else \"TRAINING\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1PjldhpdvQC4"
      },
      "source": [
        "<a name=\"base-station\"></a>\n",
        "Definindo uma classe que representa uma Base Station (BS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wjRXNOw8vU3Z"
      },
      "outputs": [],
      "source": [
        "class BaseStation():\n",
        "    bs_id = 0\n",
        "\n",
        "    def __init__(self, position):\n",
        "      self.id = BaseStation.bs_id\n",
        "\n",
        "      self.x = position[0]\n",
        "      self.y = position[1]\n",
        "\n",
        "      self.users = []\n",
        "      self.us_id = 0 #Users ID\n",
        "\n",
        "      BaseStation.bs_id += 1\n",
        "\n",
        "    def add_user(self):\n",
        "      self.users.append(User(self.us_id))\n",
        "\n",
        "      self.us_id += 1\n",
        "\n",
        "    def update_user_data(self, us_id, regs):\n",
        "      perturbed_regs = self._add_perturbation(regs)\n",
        "\n",
        "      self.users[us_id].set_data(perturbed_regs)\n",
        "\n",
        "    def _add_perturbation(self, regs):\n",
        "      regs_perturbed = regs.copy()\n",
        "\n",
        "      for reg in regs_perturbed:\n",
        "        us_x = reg[0]\n",
        "        us_y = reg[1]\n",
        "\n",
        "        #Sigma based on the distance between user and BS\n",
        "        sigma = 0.1 * distance.euclidean((us_x, us_y), (self.x, self.y))\n",
        "\n",
        "        np.random.seed(0)\n",
        "        perturb = math.sqrt(sigma) * np.random.rand(1,3)\n",
        "\n",
        "        #User's location and orientation in BS's perception\n",
        "        reg += perturb[0]\n",
        "\n",
        "      return regs_perturbed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OmV35A7VwmWg"
      },
      "source": [
        "<a name=\"user\"></a>\n",
        "Definindo uma classe que representa um Usuário (User)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c8dZtVhMxaN2"
      },
      "outputs": [],
      "source": [
        "class User():\n",
        "\n",
        "  def __init__(self, id):\n",
        "    self.id = id\n",
        "\n",
        "    torch.manual_seed(torch_seed)\n",
        "\n",
        "    self.inputs  = []\n",
        "    self.targets = []\n",
        "\n",
        "    self.model_accuracy = 0\n",
        "\n",
        "    self._set_model()\n",
        "\n",
        "  def _set_model(self):\n",
        "\n",
        "    if(current_training_model == esn_training_model):\n",
        "      self.model = EchoStateNetwork(input_size, hidden_size, output_size).to(device)\n",
        "    elif(current_training_model == gru_training_model):\n",
        "      self.model = GRUModel(input_size, hidden_size, output_size).to(device)\n",
        "    elif(current_training_model == lstm_training_model):\n",
        "      self.model = LSTMModel(input_size, hidden_size, 1, output_size).to(device)\n",
        "    else:\n",
        "      print('Model undefined')\n",
        "      1/0\n",
        "\n",
        "  def set_data(self, data):\n",
        "\n",
        "    self.test_target = data[-1]\n",
        "\n",
        "    #Removing last reg for accuracy testing target\n",
        "    data = data[:len(data) - 1]\n",
        "\n",
        "    self.test_input = data[-1]\n",
        "\n",
        "    #Removing last reg for accuracy testing input\n",
        "    data = data[:len(data) - 1]\n",
        "\n",
        "    #Reshape to match the RNN input shape\n",
        "    data = data.reshape(-1, num_user_regs_per_time, input_size)\n",
        "\n",
        "    self.inputs  = data[:-1] #All regs except the last to train\n",
        "    self.targets = data[1:].squeeze()  #All regs except the first to test\n",
        "\n",
        "  def train_model(self):\n",
        "\n",
        "    self.model.train()\n",
        "\n",
        "    # Define optimizer and loss function\n",
        "    optimizer = torch.optim.Adam(self.model.parameters(), lr=learning_rate, weight_decay=1e-7)\n",
        "    criterion = nn.MSELoss().to(device)\n",
        "\n",
        "    inputs_tensor = torch.from_numpy(self.inputs).float().to(device)\n",
        "    targets_tensor = torch.from_numpy(self.targets).float().to(device)\n",
        "\n",
        "    # Train the RNN model\n",
        "    for epoch in range(epochs):\n",
        "        outputs = self.model(inputs_tensor)\n",
        "        loss = criterion(outputs, targets_tensor)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    self._get_model_accuracy()\n",
        "\n",
        "  def _get_model_accuracy(self):\n",
        "    tracking_input = torch.from_numpy(self.test_input)\n",
        "    tracking_input = tracking_input.float().to(device)\n",
        "    tracking_input = tracking_input[None, None, :]\n",
        "\n",
        "    self.model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "      tracking_predicted = self.model(tracking_input)\n",
        "\n",
        "    self.model_accuracy = 100 - self._get_mean_absolute_percentage_error(self.test_target, tracking_predicted[:, :3])\n",
        "\n",
        "  def _get_mean_absolute_percentage_error(self, target, predicted):\n",
        "      mape_x = abs((predicted[0][0] - target[0]) / target[0]) * 100\n",
        "      mape_y = abs((predicted[0][1] - target[1]) / target[1]) * 100\n",
        "      mape_o = abs((predicted[0][2] - target[2]) / target[2]) * 100\n",
        "\n",
        "      #overall MAPE value limited by 100\n",
        "      mape = (mape_x + mape_y + mape_o) / 3\n",
        "      mape = min(mape.item(), 100)\n",
        "\n",
        "      return mape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BmulsrClxnr9"
      },
      "source": [
        "<a name=\"bs-local\"></a>\n",
        "Definindo (empíricamente) as posições das Base Stations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "80I1jbFsxtmO"
      },
      "outputs": [],
      "source": [
        "base_stations_locations = []\n",
        "\n",
        "#Empirical positions\n",
        "base_stations_locations.append([185.07, 656.36])\n",
        "base_stations_locations.append([265.54, 137.68])\n",
        "base_stations_locations.append([703.75, 448.22])\n",
        "base_stations_locations.append([948.82, 829.26])\n",
        "base_stations_locations.append([1010.93, 322.32])\n",
        "\n",
        "locations_mean = np.mean(base_stations_locations)\n",
        "locations_std  = np.std(base_stations_locations)\n",
        "\n",
        "#Normalized positions\n",
        "base_stations_locations = (base_stations_locations - locations_mean) / locations_std"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yX2EyiVdyQFU"
      },
      "source": [
        "<a name=\"dataset\"></a>\n",
        "Carregando arquivo CSV em um dataset\n",
        "\n",
        "O dataset será subdividido em 20 partes iguais para que seja possível distribuir dados de cada um dos 20 usuários presentes no dataset entre as 5 base stations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vujwqGW7ykb8"
      },
      "outputs": [],
      "source": [
        "# Load the CSV file into a Pandas DataFrame\n",
        "caminho_arquivo = dir_base + \"20_users_1_hour_sorted.csv\"\n",
        "dataset = pd.read_csv(caminho_arquivo, header=0, usecols=[\"x\", \"y\", \"o\"])\n",
        "\n",
        "dataset = dataset.to_numpy()\n",
        "\n",
        "partitions = []\n",
        "\n",
        "# Split the DataFrame into partitions\n",
        "for i in range(num_users):\n",
        "    s_row = i * rows_per_partition\n",
        "    e_row = s_row + rows_per_partition\n",
        "    partitions.append(dataset[s_row:e_row])\n",
        "\n",
        "partitions_mean = np.mean(partitions)\n",
        "partitions_std = np.std(partitions)\n",
        "\n",
        "# Normalized data\n",
        "partitions = (partitions - partitions_mean) / partitions_std"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vMlkfIgKzyEj"
      },
      "source": [
        "<a name=\"create_bs_us\"></a>\n",
        "Criando Base Stations e Users and the Server"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2rd1J-PTAcXp"
      },
      "outputs": [],
      "source": [
        "def create_scenario():\n",
        "  BaseStation.bs_id = 0\n",
        "\n",
        "  base_stations = []\n",
        "\n",
        "  for i in range(num_base_stations):\n",
        "    base_station = BaseStation(base_stations_locations[i])\n",
        "\n",
        "    for i in range(num_users):\n",
        "      base_station.add_user()\n",
        "\n",
        "    base_stations.append(base_station)\n",
        "\n",
        "  server = Server(num_users)\n",
        "\n",
        "  sModel = None\n",
        "\n",
        "  if(current_training_model == esn_training_model):\n",
        "    sModel = \"ESN RNN\"\n",
        "  elif(current_training_model == gru_training_model):\n",
        "    sModel = \"GRU RNN\"\n",
        "  elif(current_training_model == lstm_training_model):\n",
        "    sModel = \"LSTM RNN\"\n",
        "\n",
        "  print(\"Current training model: \" + sModel)\n",
        "\n",
        "  return base_stations, server"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CMjLaF3XDw82"
      },
      "source": [
        "<a name=\"predicting\"></a>\n",
        "Realizando predições..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yWB5-Y3Q3iEA"
      },
      "outputs": [],
      "source": [
        "def getting_predictions(server, base_stations):\n",
        "  #Penultimate (input) and last (target) seconds of each 10 minutes of walking not used in training\n",
        "  plot_row = 3058\n",
        "\n",
        "  for us_id, us_model in server.users_global_model.items():\n",
        "    for bs in base_stations:\n",
        "\n",
        "      user = bs.users[us_id]\n",
        "\n",
        "      tracking_input = torch.from_numpy(partitions[us_id][plot_row: plot_row + 1])\n",
        "      tracking_input = tracking_input.float().to(device)\n",
        "      tracking_input = tracking_input[None, :]\n",
        "\n",
        "      #Prediciton on global (from server) model\n",
        "      us_model.eval()\n",
        "      with torch.no_grad():\n",
        "        prediction = us_model(tracking_input).cpu().data.numpy()\n",
        "\n",
        "      #Denormalizing data\n",
        "      prediction = (prediction * partitions_std) + partitions_mean\n",
        "      target = (partitions[us_id][plot_row + 1: plot_row + 2] * partitions_std) + partitions_mean\n",
        "\n",
        "      a = (target[0][0],  target[0][1])\n",
        "      b = (prediction[0][0], prediction[0][1])\n",
        "\n",
        "      euclidean_distance = distance.euclidean(a, b)\n",
        "\n",
        "      input = (partitions[us_id][plot_row: plot_row + 1] * partitions_std) + partitions_mean\n",
        "\n",
        "      if(us_id == 0):\n",
        "        print(\"Base Station {}\".format(str(bs.id)))\n",
        "        print(\"User\", us_id, \"input\", (input[0][0], input[0][1], input[0][2]))\n",
        "        print(\"User\", us_id, \"target\", (target[0][0], target[0][1], target[0][2]))\n",
        "        print(\"User\", us_id, \"predicted\", (prediction[0][0], prediction[0][1], prediction[0][2]))\n",
        "        print(\"The Euclidean distance for user\", us_id, \"=\", euclidean_distance, \"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-LxQatbj_UQ6"
      },
      "outputs": [],
      "source": [
        "def save_user_data(user_id, data):\n",
        "  csv_file_sufix = None\n",
        "  pth_file_sufix = None\n",
        "\n",
        "  if(current_training_model == esn_training_model):\n",
        "    csv_file_sufix = \"_esn_model_tracking_data.csv\"\n",
        "    pth_file_sufix = \"_esn_model.pth\"\n",
        "  elif(current_training_model == gru_training_model):\n",
        "    csv_file_sufix = \"_gru_model_tracking_data.csv\"\n",
        "    pth_file_sufix = \"_gru_model.pth\"\n",
        "  elif(current_training_model == lstm_training_model):\n",
        "    csv_file_sufix = \"_lstm_model_tracking_data.csv\"\n",
        "    pth_file_sufix = \"_lstm_model.pth\"\n",
        "\n",
        "  csv_tracking_file = dir_base + (\"results_sbrc_models_fl-cfa/\" if fitness_filter else \"results_sbrc_models_fl-sfa/\") + str(user_id) + csv_file_sufix\n",
        "\n",
        "  with open(csv_tracking_file, 'w', newline='') as file:\n",
        "    writer = csv.writer(file)\n",
        "    writer.writerows(data)\n",
        "\n",
        "  model_path = dir_base + (\"results_sbrc_models_fl-cfa/\" if fitness_filter else \"results_sbrc_models_fl-sfa/\") + \"user_\" + str(user_id) + pth_file_sufix\n",
        "\n",
        "  torch.save(server.users_global_model[user_id].state_dict(), model_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hCNZxlOAPg3n"
      },
      "outputs": [],
      "source": [
        "def prepare_predictions_for_save(server):\n",
        "\n",
        "  for user_id in range(0, 20):\n",
        "    user_id = user_id\n",
        "\n",
        "    #Penultimate (input) and last (target) seconds of each 10 minutes of walking not used in training\n",
        "    plot_row = 3058\n",
        "\n",
        "    user_tracking_data = []\n",
        "\n",
        "    #Ten minutes\n",
        "    for i in range(1,11):\n",
        "        input = torch.from_numpy(partitions[user_id][plot_row: plot_row + 1])\n",
        "        input = input.float().to(device)\n",
        "        input = input[None, :]\n",
        "\n",
        "        server.users_global_model[user_id].eval()\n",
        "        with torch.no_grad():\n",
        "          prediction = server.users_global_model[user_id](input).cpu().data.numpy()\n",
        "\n",
        "        #Denormalizing data\n",
        "        prediction = (prediction * partitions_std) + partitions_mean\n",
        "        target = (partitions[user_id][plot_row + 1: plot_row + 2] * partitions_std) + partitions_mean\n",
        "\n",
        "        input =  (input.cpu().data.numpy() * partitions_std) + partitions_mean\n",
        "        user_tracking_data.append([input, target, prediction])\n",
        "\n",
        "        plot_row += 60 #plus 1 minute\n",
        "\n",
        "    save_user_data(user_id, user_tracking_data)\n",
        "\n",
        "  #Input data have to be normalized before prediction (using the models)\n",
        "  params_path = dir_base + (\"results_sbrc_models_fl-cfa/\" if fitness_filter else \"results_sbrc_models_fl-sfa/\") + \"normalize_params.pth\"\n",
        "\n",
        "  torch.save({'mean': partitions_mean, 'std': partitions_std}, params_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vWUhRaoRFunn"
      },
      "source": [
        "<a name=\"training\"></a>\n",
        "A cada *round* uma quantidade de registros da caminhada do User (localização, orientação) serão adicionados a cada uma das Base Station (BS). O objetivo é simular o aumento de registros em cada BS ao longo do tempo (*rounds*).\n",
        "Foi definido que a cada *round* cada BS será incrementada com 5 segundos (5 registros de caminhada) de cada User."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rJ560CAqKAK7"
      },
      "outputs": [],
      "source": [
        "columns = ['Rodada', 'ID_Usuario', 'Acuracia', 'Status']\n",
        "\n",
        "def gather_accuracy_per_round(df, server):\n",
        "      for k, v in server.users_global_model_accuracy.items():\n",
        "        status = \"READY\" if v >= accuracy_threshold else \"TRAINING\"\n",
        "\n",
        "        temp_df = pd.DataFrame({\n",
        "            'Rodada': [round],\n",
        "            'ID_Usuario': [k],\n",
        "            'Acuracia': [v],\n",
        "            'Status': [status]\n",
        "        })\n",
        "\n",
        "        df = pd.concat([df, temp_df], ignore_index=True)\n",
        "\n",
        "def save_accuracy_per_round(df):\n",
        "  csv_accuracy_per_round = dir_base + (\"results_sbrc_models_fl-cfa/\" if fitness_filter else \"results_sbrc_models_fl-sfa/\") + \"csv_accuracy_per_round\"\n",
        "\n",
        "  if(current_training_model == gru_training_model):\n",
        "    csv_accuracy_per_round += \"_gru.csv\"\n",
        "  elif(current_training_model == esn_training_model):\n",
        "    csv_accuracy_per_round += \"_esn.csv\"\n",
        "  else:\n",
        "    csv_accuracy_per_round += \"_lstm.csv\"\n",
        "\n",
        "  df.to_csv(csv_accuracy_per_round, index=False)\n",
        "\n",
        "for current_training in range(1,4):\n",
        "  df = pd.DataFrame(columns=columns)\n",
        "\n",
        "  current_training_model = current_training\n",
        "\n",
        "  base_stations, server = create_scenario()\n",
        "\n",
        "  e_row = num_user_regs\n",
        "\n",
        "  for round in range(rounds):\n",
        "    for base_station in base_stations:\n",
        "      for user in base_station.users:\n",
        "        #updating base stations (and users) with perturbed tracking data\n",
        "        base_station.update_user_data(user.id, partitions[user.id][0: e_row])\n",
        "\n",
        "        user.train_model()\n",
        "\n",
        "    #Aggregating models and checking accuracy\n",
        "    server.update_global_model(base_stations)\n",
        "\n",
        "    e_row += num_user_regs\n",
        "\n",
        "    gather_accuracy_per_round(df, server)\n",
        "\n",
        "    if(server.can_stop_federation()):\n",
        "      print(\"Round {}\".format(round))\n",
        "      save_accuracy_per_round(df)\n",
        "      getting_predictions(server, base_stations)\n",
        "      prepare_predictions_for_save(server)\n",
        "      break"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}